这 10 道题目（包含 1 道关于 CloudFormation 删除 S3 桶的题目）涵盖了 **AWS Lambda 与 DynamoDB 集成**、**VMware 迁移**、**ECS 数据加密**、**VPC DNS**、**Redshift 扩展**、**S3 审计**、**ECR 组织访问**、**AWS Backup**、**跨账户 AssumeRole**、**Storage Gateway 归档**。

特别是 **Q329 (CloudFormation S3 Bucket Deletion)** 和 **Q327 (Session Data Decoupling)** 是 SAP 考试中的经典“坑”。

让我们开启 **逐题秒杀模式**！

---

#### 📝 [321/529] EC2 SSH 唯一密钥审计 (Secrets Manager + Lambda)

**1. 🕵️‍♂️ 题眼与约束分析**
* **场景：** 每日模拟，数百实例。
* **需求：**
    1.  **No EC2 instance can share the same SSH key** (每台实例唯一密钥)。
    2.  All connections logged in **CloudTrail**。
* **操作：** 工程师偶尔 SSH 连接排错。

**2. ⚡ 秒杀思路**
* **唯一密钥生成：**
    * 启动实例时生成密钥？如果在 Launch Template 里指定 Key Pair，那所有实例都一样。必须在启动后或启动时动态生成。
    * **Secrets Manager + Lambda (D):** 使用 Lambda 动态生成 SSH 密钥对，存入 Secrets Manager，并通过 **SSM Session Manager** (或 Run Command) 将公钥注入到实例。这样每个实例都有独立的密钥。
    * 选项 A: 启动实例时生成密钥？谁生成？User Data？那私钥怎么给工程师？存 Secrets Manager 是对的，但流程没 D 详细。
* **审计连接：**
    * 题目要求 Log connections in CloudTrail。
    * **GetSecretValue (A/D):** 获取密钥的操作会被 CloudTrail 记录。这满足了“记录连接（尝试）”的需求。
    * **EC2 Instance Connect (C):** 使用 `SendSSHPublicKey` API。这个 API 调用也会被 CloudTrail 记录。且 EIC 实际上是把临时公钥推送到实例，这天然满足“唯一密钥”（因为是临时的、针对用户的）。
    * **A vs C vs D:**
        * A: "Launch new instances... generate separate key". EC2 启动 API 不支持自动生成不同密钥（除非写复杂脚本）。
        * D: Lambda + SSM + Secrets Manager Rotation。这个方案可以实现“每天轮换”，且 Secrets Manager 可以存几百个密钥。工程师获取密钥时会有 CloudTrail 记录。
        * C: EC2 Instance Connect。不需要管理密钥！用户生成临时密钥，推送公钥，连接。每次连接都是唯一的（针对该次会话）。且 `SendSSHPublicKey` 被审计。
        * **关键点：** C 选项说 "Launch without any SSH key"。这是最佳实践。EIC 允许无密钥启动，按需推送。这比 D（维护数百个密钥并轮换）要轻量得多，且完全符合“无共享密钥”和“审计”要求。
        * **但是，** 题目说 "No EC2 instance can share the same SSH key"。C 选项其实根本没有持久化的 SSH Key。这在语义上满足（因为没有共享的）。
        * D 选项是一个非常“重”的方案（几百个 Secrets）。
        * **让我们再看 D:** "Configure Secrets Manager to rotate daily". 题目是“偶尔连接”。每天轮换几百个没用的密钥？浪费。
        * **C 选项 EC2 Instance Connect** 是最现代、最符合云原生的 SSH 访问方式，零密钥管理，全审计。
        * **锁定 C?**
        * 等等，C 选项最后说 "Use browser-based SSH client"。这没问题。
        * **让我们再看 A/D 的描述：** D 明确提到了 "Create new Lambda... invoke SSM to set key"。这是可行的。但维护成本高。
        * 如果题目是想考 Secrets Manager 的用法，D 是合理的。
        * 但如果考的是“最佳实践”，C 是首选。
        * **然而，** 许多旧题库或特定场景下，**A/D** 这种“显式管理密钥”的方案也被视为正确，特别是当企业有“密钥轮换”的合规要求时。但题目只说 "No instance share same key"。
        * **仔细对比 C 和 D:**
            * C: 无密钥启动。按需推送。
            * D: 自动生成唯一密钥，存 Secrets Manager，SSM 注入。
            * 两者都行。但 D 的“每天自动轮换”对于数百个实例来说，API 调用量和复杂性都很高。
            * C 的 `SendSSHPublicKey` 是针对**用户**的，不是针对**实例**的（虽然推送到特定实例）。题目要求 "No **EC2 instance** can share the same SSH key"。这通常指实例级别的密钥。
            * 实际上，C 方案中，实例本身没有持久密钥。这满足要求。
            * 但是，AWS 考试中有时偏爱 **Secrets Manager** 方案（尤其是涉及到“存储密钥”和“轮换”的上下文）。
            * **不过，** 让我们看 C 的一个潜在问题：如果实例是私有子网且没公网 IP，Browser-based SSH 需要 Endpoint 支持。
            * **题目还有一个限制：** "All connections must be logged"。EIC 的 `SendSSHPublicKey` 记录的是“推送密钥”的动作，不是 SSH 连接本身的流量（虽然时间点接近）。而 SSM Session Manager (D 中用来注入密钥，或者直接用来连接) 提供了更完整的会话记录。
            * **Wait, D 只是用 SSM 注入密钥，连接还是用 SSH 客户端。**
            * 此时获取密钥的 API `GetSecretValue` 会被记录。这也算“记录了连接意图”。
            * **关键区别：** EC2 Instance Connect (C) 是 ephemeral key。Secrets Manager (D) 是 persistent (rotated) key。题目说 "No EC2 instance can share the same SSH key"。这通常暗示每个实例有一个**固定的、唯一的**密钥。
            * 如果是这样，D 更符合字面意思。且 D 确实能做到。
            * **但是，** C 是 AWS 推荐的。
            * **让我们看 A:** 手动生成？不现实。
            * **最终判定：** 在 AWS Security Specialty 或 SAP 中，**EC2 Instance Connect (C)** 通常是 SSH 访问的首选答案，因为它消除了密钥管理。
            * **但是！** 题目可能是一个旧题，或者强调“每个实例有单独密钥”。如果必须选一个“管理密钥”的方案，D 是自动化的。
            * **我倾向于 C**。因为“No Key”是“Unique Key”的终极形式。且 CloudTrail 审计要求也能满足。

**3. ✅ 正确选项解析 (选项 C)**
* **EC2 Instance Connect:** 消除长期密钥，按需推送临时公钥，CloudTrail 记录 API 调用，满足审计和唯一性要求（实际上是无密钥）。

**5. 📚 核心考点:** EC2 SSH 访问的安全最佳实践 (Instance Connect vs Key Management)。

---

#### 📝 [322/529] VPC 解析本地 DNS (Route 53 Resolver)

**1. 🕵️‍♂️ 题眼与约束分析**
* **场景：** AWS VPC (EC2) 需要解析 On-prem AD Domain。
* **连接：** Direct Connect。
* **目标：** **Least administrative overhead**。

**2. ⚡ 秒杀思路**
* **DNS 解析：**
    * **Route 53 Resolver Outbound Endpoint (C):** 它是专门设计用来让 VPC 内的资源解析本地 DNS 的。通过配置 **Forwarding Rule** (条件转发)，将特定域名（如 `corp.local`）的查询转发给本地 DNS 服务器 IP。这是托管服务，运维最少。
* **选项对比：**
    * 选项 A (EC2 DNS): 自建 BIND/Unbound？维护麻烦，高可用要自己搞。
    * 选项 B (Private Hosted Zone + NS): PHZ 的 NS 记录不能直接指向本地 IP（除非通过 Resolver）。而且这是让本地解析 AWS，还是 AWS 解析本地？题目是 AWS 解析本地。PHZ 是托管 AWS 域名的。
    * 选项 D (AD Trust): 部署 AD 域控？太重了，只为了 DNS 解析没必要。
    * **选项 C:** Route 53 Resolver + Outbound Endpoint + Conditional Forwarding。标准答案。
    * **锁定 C。**

**3. ✅ 正确选项解析 (选项 C)**
* **Route 53 Resolver Outbound Endpoint:** 混合云 DNS 解析的桥梁。

**5. 📚 核心考点:** 混合云 DNS 架构 (Outbound Endpoint)。

---

#### 📝 [323/529] 实时环境数据存储 (DynamoDB vs TimeStream?)

**1. 🕵️‍♂️ 题眼与约束分析**
* **数据：** Environmental data (Sensor streams). JSON format.
* **需求：** Real-time (实时), **No fixed schema** (无固定模式).
* **目标：** Store data.

**2. ⚡ 秒杀思路**
* **数据特征：** JSON, No fixed schema $\rightarrow$ **NoSQL**。
* **服务选型：**
    * **DynamoDB (B):** Serverless NoSQL，支持 JSON 文档，无 Schema 限制（Schemaless），高并发写入。适合传感器数据。Kinesis Data Streams 可以直接写 DynamoDB（通过 Lambda 或 Firehose 转换，虽然选项 B 说 "Kinesis Data Streams send to DynamoDB"，通常需要中间件，但逻辑上是通的）。
    * **Redshift (A):** 关系型数仓，Schema 固定（虽然支持 Super 类型，但主要还是结构化）。Firehose 可以直连。
    * **Aurora (C):** 关系型，Schema 固定。
    * **Keyspaces (D):** Cassandra 兼容。虽然也是 NoSQL，但 DynamoDB 是 AWS 原生首选。且 Firehose 直连 Keyspaces 不是原生支持的（需要 HTTP Endpoint）。
* **选项 B 的合理性：** "Use Kinesis Data Streams send to DynamoDB"。这通常意味着 Kinesis -> Lambda -> DynamoDB。这是经典的流式写入架构。DynamoDB 完美契合 "No fixed schema" 和 "Real-time" 需求。
* **注意：** 题目没有 TimeStream 选项。如果有，TimeStream 是时序数据的首选。在没有 TimeStream 的情况下，DynamoDB 是最佳替代。
* **锁定 B。**

**3. ✅ 正确选项解析 (选项 B)**
* **DynamoDB:** NoSQL 数据库，支持灵活 Schema (JSON)，适合高并发传感器数据。

**5. 📚 核心考点:** 实时非结构化/半结构化数据存储选型。

---

#### 📝 [324/529] MongoDB 迁移与私有网络 (DocumentDB)

**1. 🕵️‍♂️ 题眼与约束分析**
* **源：** Legacy App + MongoDB。
* **迁移：** To AWS。
* **约束：**
    1.  Private Subnets (No Internet)。
    2.  Encrypted connections。
    3.  Scale on demand。
* **数据库：** MongoDB Key-Value store。

**2. ⚡ 秒杀思路**
* **兼容性：** **Amazon DocumentDB** 是 MongoDB 兼容的。迁移成本最低。DynamoDB (B/C) 虽然也是 KV，但 API 不兼容，需要重写代码。
* **连接方式：**
    * DocumentDB 部署在 VPC 内。
    * **Cluster Endpoint (D):** 读写主节点的入口。
    * **Instance Endpoint (A):** 连接特定实例，通常用于只读或诊断，不推荐作为应用的主连接点（因为故障转移后实例角色会变）。
* **选项对比：**
    * **选项 D:** DocumentDB + Cluster Endpoint。这是标准用法。DocumentDB 默认支持加密（TLS）和 VPC 部署。
    * **选项 A:** Instance Endpoint 错误。
    * **选项 B/C:** DynamoDB 需要重写应用。
    * **锁定 D。**

**3. ✅ 正确选项解析 (选项 D)**
* **DocumentDB:** MongoDB 兼容，VPC 部署，Cluster Endpoint 提供高可用连接。

**5. 📚 核心考点:** MongoDB 迁移目标选型与连接端点。

---

#### 📝 [325/529] MongoDB 到 DocumentDB 迁移 (DMS)

**1. 🕵️‍♂️ 题眼与约束分析**
* **源：** On-prem MongoDB (via DX).
* **目标：** Amazon DocumentDB。
* **任务：** Migrate (迁移)。

**2. ⚡ 秒杀思路**
* **迁移工具：** **AWS DMS (Database Migration Service)** 支持 MongoDB 作为源，DocumentDB 作为目标。
* **迁移模式：**
    * 支持 **CDC (Change Data Capture)**，实现最小停机时间迁移。
    * $\rightarrow$ **选中 B**。
* **排除法：**
    * A (EC2 MongoDB): 这是 Rehost，不是迁移到 DocumentDB。
    * C (Data Pipeline): 过时工具，主要用于批处理数据移动，不支持数据库 CDC。
    * D (Glue): Glue 是 ETL 工具，虽然能搬数据，但不是为了数据库实时迁移（CDC）设计的，且配置复杂。DMS 是专业的。
* **锁定 B。**

**3. ✅ 正确选项解析 (选项 B)**
* **DMS:** 数据库同构/异构迁移的标准工具，支持 CDC。

**5. 📚 核心考点:** 数据库迁移工具 DMS 的适用场景。

---

#### 📝 [326/529] 托管 AD 与安全配置 (AWS Managed Microsoft AD)

**1. 🕵️‍♂️ 题眼与约束分析**
* **需求：**
    1.  Windows EC2 加入 AD 域。
    2.  Enhanced security (MFA)。
    3.  **Managed AWS Service** (托管服务)。
* **管理工具：** 需要配置域安全。

**2. ⚡ 秒杀思路**
* **目录服务：**
    * **AWS Managed Microsoft AD (A/B):** 真正的 Active Directory，支持组策略、信任关系、MFA 等高级功能。
    * **Simple AD (C/D):** 基于 Samba 4，不支持 MFA，功能有限。排除。
* **管理方式：**
    * 要管理 AD（如配置 GPO、用户），需要在一个加入了该域的 Windows 机器上安装 RSAT 工具。
    * **Amazon WorkSpaces (A):** 可以作为管理机，但这需要额外费用和配置。
    * **EC2 Instance (B):** 启动一个 Windows EC2，加入域，安装 RSAT。这是最标准、最轻量的管理方式（Bastion/Jumpbox）。
    * 题目问 "Which solution meets these requirements"。A 和 B 都可以。
    * 但是，**MFA** 是关键。AWS Managed Microsoft AD 原生支持 MFA。
    * 既然 A 和 B 都能用 Managed AD，区别在于管理端点。
    * **EC2 (B)** 是最通用的管理方式。WorkSpaces (A) 也是一种方式，但通常用于最终用户桌面。为了“配置域安全”，开一个 EC2 就够了。
    * **等等，** 题目说 "Connect to and use... for domain security configuration tasks"。这通常指安装 RSAT。
    * 实际上，AWS 官方文档中管理 Managed AD 的标准步骤就是：Launch EC2 -> Join Domain -> Install RSAT。
    * **锁定 B。**

**3. ✅ 正确选项解析 (选项 B)**
* **AWS Managed Microsoft AD:** 支持 MFA 和域加入。
* **EC2 + RSAT:** 标准的管理方式。

**5. 📚 核心考点:** AWS 托管 AD 的选型与管理。

---

#### 📝 [327/529] 数据库解耦与会话存储 (RDS + DynamoDB)

**1. 🕵️‍♂️ 题眼与约束分析**
* **数据类型：** Structured Product Data (结构化产品数据) + Ephemeral Session Data (临时会话数据)。
* **需求：**
    1.  **Decouple** (解耦)。
    2.  **Replication to another region** (跨区灾备)。
    3.  **Highest performance** (最高性能)。

**2. ⚡ 秒杀思路**
* **产品数据 (Structured):** 适合关系型数据库。**RDS (A/B/D)**。支持跨区域 Read Replica。
* **会话数据 (Ephemeral, High Performance):**
    * **ElastiCache (B):** 内存级性能，最快。支持 Global Datastore (跨区域复制)。
    * **DynamoDB (C/D):** 性能也很好，支持 Global Tables (跨区域)。
* **对比 B 和 D:**
    * **B (ElastiCache Global Datastore):** 专门针对 Redis 的跨区域复制。对于 Session 这种读写极其频繁且需要极低延迟的数据，Redis 是首选。
    * **D (DynamoDB Global Tables):** 也非常好。
    * **性能之争：** Redis (微秒级) > DynamoDB (毫秒级)。题目要求 "Highest performance"。
    * **但是，** 题目还要求 "Disaster Recovery"。DynamoDB Global Table 是多活的，DR 切换极快。Redis Global Datastore 也是。
    * **关键细节：** 选项 B 说 "ElastiCache for Memcached"。**Memcached 不支持 Global Datastore** (跨区域复制)！只有 **Redis** 支持。这是一个巨大的陷阱。
    * **重新审视 B:** 如果是 Redis，B 可能是对的。但如果是 Memcached，B 直接排除（无法满足 DR 复制需求）。
    * **查看选项 C:** "Two DynamoDB Global Tables"。全用 NoSQL？产品数据是结构化的，RDS 更好。
    * **查看选项 D:** "RDS for Product... DynamoDB Global Table for Session"。这是一个非常稳健的组合。RDS 解决结构化数据，DynamoDB Global Table 解决 Session 的高性能和跨区域复制。虽然 Redis 更快，但 Memcached (B) 不行，且 DynamoDB 对于 Session 也是业界标准。
    * **结论：** 只能选 **D**。因为 B 用的 Memcached 不支持跨区域复制。

**3. ✅ 正确选项解析 (选项 D)**
* **RDS:** 结构化数据。
* **DynamoDB Global Tables:** Session 数据的高性能跨区域同步方案。

**5. 📚 核心考点:** ElastiCache (Memcached vs Redis) 的能力区别与 Session 存储选型。

---

#### 📝 [328/529] 开发账户成本控制 (Control Tower + Custom Control)

**1. 🕵️‍♂️ 题眼与约束分析**
* **环境：** Control Tower, Organizations。
* **目标：** Developer accounts (开发账户)。
* **需求：**
    1.  **Limit EC2/RDS to burstable types** (限制只能用 T 系列)。
    2.  **Block unused services** (禁止其他服务)。
* **工具：** Control Tower / SCP。

**2. ⚡ 秒杀思路**
* **Control Tower 护栏 (Controls):**
    * Control Tower 提供了 **Preventive Controls** (基于 SCP) 和 **Detective Controls** (基于 Config)。
    * 题目要求 "Only allow... Block others"。这是**拦截/禁止**，属于 Preventive。
* **实现方式：**
    * **SCP (A):** SCP 是 Organizations 的原生功能，可以实现 Region 限制、实例类型限制、服务白名单。
    * **Control Tower Custom Control (C):** Control Tower 允许你定义自定义护栏（底层也是 SCP）。
    * **选项对比：**
        * A: "Create custom SCP in Organizations". 这在 Control Tower 环境下是允许的，但 Control Tower 推荐通过其控制台或机制管理（如 CfCT）。
        * C: "Create custom **preventive control (guardrail)** in Control Tower". 这更符合 Control Tower 的语境。虽然底层是 SCP，但在 CT 架构题中，使用 "Guardrail/Control" 术语更准确。
        * **A vs C:** A 说 "Create SCP in Organizations"。C 说 "Create control in Control Tower"。如果你用了 Control Tower，最好通过 Control Tower 管理（否则可能漂移）。C 选项体现了“在 CT 中管理”的意图。
    * **关键：** 题目明确说 "Company uses AWS Control Tower...". 所以应该选 Control Tower 相关的选项。
    * **锁定 C。**

**3. ✅ 正确选项解析 (选项 C)**
* **Custom Preventive Control:** 在 Control Tower 框架下部署 SCP，实现强制合规。

**5. 📚 核心考点:** Control Tower 环境下的自定义护栏 (SCP) 管理。

---

#### 📝 [329/529] CloudFormation 删除失败 (S3 非空)

**1. 🕵️‍♂️ 题眼与约束分析**
* **问题：** CloudFormation 删除 Stack 失败，原因是 **S3 Bucket failed to delete**。
* **原因：** S3 桶**非空**时，CloudFormation 无法删除它（这是保护机制）。
* **需求：** Resolve issue, **No major architectural changes**。

**2. ⚡ 秒杀思路**
* **解决方案 1 (自定义资源):**
    * 使用 Lambda-backed **Custom Resource** (A)。
    * 逻辑：在删除 Stack 时，Lambda 先清空桶 (`DeleteObjects`)。
    * 依赖：`DependsOn` 确保 S3 桶在被 CFN 删除前先被 Lambda 清空。
    * 这是一个经典的解决“非空桶删除”的模式。
* **解决方案 2 (S3 策略):**
    * 选项 C (Lifecycle): 生命周期删除需要时间（最少 1 天），无法解决“现在就要删除 Stack”的报错。
    * 选项 D (DeletionPolicy): `DeletionPolicy: Delete` 是默认的。如果是 `Retain`，Stack 删除会忽略桶（保留它），这能解决 Stack 删除失败的问题，但会留下垃圾资源。题目没说允许留垃圾，只是要解决失败。
* **最佳实践：** 为了自动化彻底删除，**Option A** (Custom Resource to empty bucket) 是最干净的。
* **但是，** 如果题目允许保留桶？"Resolve this issue"。通常指让 Stack 删除成功。
* **比较 A 和 D:**
    * A 是完美的“清空并删除”。
    * D 的 `DeletionPolicy` 只是属性，不能强行删非空桶。
    * 实际上，CloudFormation 并没有一个简单的属性叫 "ForceDeleteNonEmptyBucket"。必须用 Custom Resource。
* **锁定 A。**

**3. ✅ 正确选项解析 (选项 A)**
* **Custom Resource (Lambda):** 只有通过代码（Lambda）先清空桶，CloudFormation 才能成功删除桶资源。

**5. 📚 核心考点:** CloudFormation 删除非空 S3 桶的解决方案。

---

#### 📝 [330/529] 游戏后端低延迟重构 (API Gateway + DynamoDB)

**1. 🕵️‍♂️ 题眼与约束分析**
* **现状：** REST API (VMs), File Storage (Session), Throttling via API Keys。
* **痛点：** Server capacity insufficient (容量不足), High latency (延迟高)。
* **需求：**
    1.  Handle varying load (应对波动)。
    2.  Low latency data access (低延迟数据)。
    3.  **API model unchanged** (API 模型不变)。
    4.  Support API Keys throttling (支持 API Key 限流)。

**2. ⚡ 秒杀思路**
* **API 层：**
    * **API Gateway (C):** 原生支持 REST API、**API Keys**、**Throttling**。完美替代现有逻辑。
    * AppSync (D): GraphQL，改变了 API 模型。
    * ALB/NLB (A/B): 不支持原生的 API Key 限流（需要自己写逻辑）。
* **计算层：**
    * **Lambda (C):** Serverless，自动扩缩容，适合波动负载。
* **存储层 (Session):**
    * **DynamoDB (C):** 低延迟 KV 存储，适合 Session。On-demand 模式适合波动负载。
* **选项 C:** API Gateway + Lambda + DynamoDB。这是 Serverless 游戏后端的标准答案。
* **锁定 C。**

**3. ✅ 正确选项解析 (选项 C)**
* **API Gateway:** 解决限流和接口托管。
* **DynamoDB:** 解决低延迟数据访问。

**5. 📚 核心考点:** 游戏后端 Serverless 重构方案。

---
**小结：**
这组题目的 **CloudFormation S3 删除**、**MongoDB 迁移**、**Control Tower** 都是实战中的“坑”。

**恭喜你，330 题！**