这 10 道题目涵盖了 **MFA 保护 (ALB+Cognito)**、**CloudFormation 权限错误**、**Lambda 数据库连接池 (RDS Proxy)**、**IoT JITP**、**CI/CD 蓝绿部署**、**多租户成本分摊**、**S3 防勒索 (Object Lock)**、**ALB 认证集成**、**Storage Gateway 迁移**、**跨区域数据传输优化**。

特别是 **Q425 (File Gateway Migration)** 和 **Q427 (CloudFront Custom Header)** 是 SAP 考试中的经典迁移与安全题。

让我们开启 **逐题秒杀模式**！

---

#### 📝 [421/529] Serverless 架构性能故障排查 (DynamoDB Throttling)

**1. 🕵️‍♂️ 题眼与约束分析**
* **故障现象：** Response times significantly longer (响应变慢)。
* **日志分析：**
    * API Gateway: 20% Errors.
    * Lambda: 1% Throttles, 10% Errors.
    * Logs indicate errors when calling **DynamoDB**.
* **结论：** 瓶颈在 DynamoDB。Lambda 错误是因为 DynamoDB 调用失败/超时。
* **目标：** Improve response times。

**2. ⚡ 秒杀思路**
* **DynamoDB 瓶颈：** 市场活动导致流量激增，DynamoDB 遭遇 **Throttling** (Read/Write Capacity Exceeded)。这会导致 Lambda 重试、超时，最终导致 API Gateway 错误和高延迟。
* **解决方案：**
    * **DynamoDB Auto Scaling (B):** 自动根据负载调整 RCU/WCU。这是解决不可预测流量导致数据库瓶颈的标准方法。
    * 增加 Lambda 并发 (A): 只会让 DynamoDB 被打得更惨。
    * 增加 API Gateway 限制 (C): 只是放行更多流量，DynamoDB 死得更快。
    * 重新设计表 (D): 长期可能需要，但“启用 Auto Scaling”是立即生效且针对“容量不足”的直接修复。
* **锁定 B。**

**3. ✅ 正确选项解析 (选项 B)**
* **DynamoDB Auto Scaling:** 动态调整吞吐量，解决数据库层面的限流问题。

**5. 📚 核心考点:** Serverless 架构中数据库层（DynamoDB）的扩容策略。

---

#### 📝 [422/529] 本地 Linux 应用快速迁移 (EC2 + EFS)

**1. 🕵️‍♂️ 题眼与约束分析**
* **源：** 3 Linux VMs, Shared file storage, Load Balancer.
* **目标：** Migrate to AWS **ASAP** (尽快).
* **需求：** High Availability, **Least architectural changes** (最少改动).

**2. ⚡ 秒杀思路**
* **最少改动迁移：**
    * **Rehost (Lift & Shift):** 将 VM 迁移到 **EC2**。
    * **共享存储：** 本地共享文件存储对应 AWS 的 **EFS** (Linux 原生 NFS)。
    * **负载均衡：** 对应 **ALB**。
    * 这个组合（EC2 + EFS + ALB）与源架构（VM + Shared File + LB）几乎一一对应，改动最小。
* **选项对比：**
    * A (ECS Fargate + S3): 需要容器化（改动大），且 S3 不是文件系统（需要改代码）。
    * C (EKS Fargate + FSx Lustre): Kubernetes 复杂度高，Lustre 是 HPC 用的。
    * D (EBS + CRR): EBS 是块存储，不能跨实例共享（除了 Multi-Attach 限制多）。CRR 是跨区域复制，不是同区域共享。
    * **B:** EC2 (Multi-AZ) + EFS + ALB。完美匹配。
* **锁定 B。**

**3. ✅ 正确选项解析 (选项 B)**
* **EC2:** 对应 VM。
* **EFS:** 对应共享文件存储。
* **ALB:** 对应负载均衡器。

**5. 📚 核心考点:** 传统三层架构（Linux + 共享文件）的 AWS 迁移映射。

---

#### 📝 [423/529] 本地网络依赖发现与可视化 (Discovery Agent)

**1. 🕵️‍♂️ 题眼与约束分析**
* **任务：** Collect network dependencies (收集网络依赖)。
* **输出：** **Diagram** showing IP, hostnames, connections (图表)。
* **环境：** Linux VMware VMs。

**2. ⚡ 秒杀思路**
* **工具：** **AWS Application Discovery Service (ADS)**。
* **模式：**
    * **Agentless:** 只能看 VM 配置，**看不到** 详细网络连接。
    * **Agent-based:** 安装在 OS 内，能抓取 TCP 连接，生成详细依赖图。
* **可视化：** ADS 数据集成到 **AWS Migration Hub**，可以在那里查看网络图（Network Diagram）。
* **选项对比：**
    * **A:** Install **Discovery Agent**... Grant permission to use Migration Hub network diagram. 这是正解。
    * B: Agentless Collector? 看不到网络图。
    * C/D: MGN Agent? MGN 是迁移工具，虽然也有复制功能，但 ADS Agent 才是专门做“发现与依赖映射”的。且 C/D 的后半部分描述（自己生成图表？）不如 Migration Hub 原生功能直接。
* **锁定 A。**

**3. ✅ 正确选项解析 (选项 A)**
* **Discovery Agent:** 收集详细网络依赖。
* **Migration Hub:** 可视化依赖关系图。

**5. 📚 核心考点:** 迁移评估阶段的网络依赖发现工具。

---

#### 📝 [424/529] Lambda 数据库连接耗尽 (RDS Proxy)

*(注：这题与 Q413 类似，但换了个问法)*

**1. 🕵️‍♂️ 题眼与约束分析**
* **架构：** Lambda + RDS MySQL。
* **故障：** High load -> Many DB connections -> Slow response (连接数过多导致慢)。
* **目标：** Improve scalability and availability。

**2. ⚡ 秒杀思路**
* **问题：** Lambda 扩容导致连接风暴。
* **解法：** **RDS Proxy (D)**。
    * 连接池复用。
    * 提高数据库效率。
    * 自动处理故障转移（提高可用性）。
* **其他选项：**
    * A (CloudWatch Action): 添加 Read Replica 需要时间，且没解决连接数过多的核心问题（写连接还是多，读连接也没池化）。
    * B (Aurora + Connection Pool in Lambda): 在 Lambda 代码里做连接池（全局变量）只能复用**单个**容器内的连接，无法解决**并发容器数**激增带来的总连接数问题。
    * C (Aurora + Route 53): 没解决连接数。
    * **D:** Migrate to Aurora (更好) + **RDS Proxy** (关键)。
* **锁定 D。**

**3. ✅ 正确选项解析 (选项 D)**
* **RDS Proxy:** 解决 Serverless 应用的数据库连接管理难题。

**5. 📚 核心考点:** Lambda 直连关系型数据库的痛点与解法 (RDS Proxy)。

---

#### 📝 [425/529] 逐步迁移 SMB 数据到 S3 (File Gateway)

**1. 🕵️‍♂️ 题眼与约束分析**
* **源：** On-prem SMB share。
* **目标：** **Amazon S3**。
* **过渡期需求：** App must continue accessing via **SMB** until rewritten (在重写代码前，应用仍需通过 SMB 访问数据，但数据要已经在 S3 上)。
* **任务：** Migrate data to AWS, allow access.

**2. ⚡ 秒杀思路**
* **核心工具：** **AWS Storage Gateway (S3 File Gateway)**。
    * **功能：** 它提供一个 **SMB 接口** 给本地应用，但后端数据存储在 **S3** 中。
    * **迁移流程：**
        1.  部署 File Gateway。
        2.  创建 File Share (映射到 S3 桶)。
        3.  将本地数据复制到 File Gateway 的 SMB 共享。
        4.  数据自动上传到 S3。
        5.  本地应用继续读写 File Gateway (SMB)，直到代码重写为 S3 API。
* **选项对比：**
    * A (DataSync to FSx): 目标是 S3，不是 FSx。
    * B (Copy to S3): 直接复制到 S3？那本地应用怎么通过 SMB 访问？应用还没改代码呢。
    * C (SMS): 迁移服务器？不是迁移数据存储。
    * **D:** Deploy **S3 File Gateway**. Create file share mapped to S3 bucket. Copy data to gateway. 这完美满足了“数据上 S3”和“应用仍用 SMB”的双重需求。
* **锁定 D。**

**3. ✅ 正确选项解析 (选项 D)**
* **S3 File Gateway:** 混合云存储桥梁，允许传统应用通过 SMB/NFS 访问 S3 对象。

**5. 📚 核心考点:** 依赖文件接口的遗留应用迁移至 S3 的过渡方案。

---

#### 📝 [426/529] 全球低延迟 API (Global Database + Edge Compute)

**1. 🕵️‍♂️ 题眼与约束分析**
* **应用：** 票务扫描（条形码验证）。
* **部署：** 3 Regions (DB hosted in 3 regions)。
* **需求：** **Lowest latency**。
* **逻辑：** Validate -> Update DB (Write)。

**2. ⚡ 秒杀思路**
* **数据层：**
    * 3 个区域都要写（标记条形码为已用）。
    * **Aurora Global Database (A/B):** 是一写多读（Write Forwarding 稍慢）。
    * **DynamoDB Global Tables (C/D):** **多主 (Multi-Master)**，任何区域都能写，毫秒级同步。对于这种简单的键值更新（标记状态），DynamoDB 是最佳选择。
* **计算层：**
    * 要最低延迟，计算逻辑越靠近用户越好。
    * **Lambda@Edge / CloudFront Functions (D):** 在 CloudFront 边缘节点运行代码。
    * **CloudFront Function** 只能处理轻量逻辑，不能访问网络（不能连 DynamoDB）。
    * **Lambda@Edge** 可以访问网络（连 DynamoDB Global Table）。
    * 架构：CloudFront (Edge) -> Lambda@Edge -> DynamoDB Global Table (Local Region)。
    * 或者：CloudFront (Edge) -> Regional API Gateway/Lambda -> DynamoDB。
* **选项对比：**
    * A (Aurora + ECS + GA): 架构重，Aurora 写延迟（跨区）比 DynamoDB 本地写高。
    * B (Aurora + EKS + CF): 同上。
    * C (DynamoDB + CloudFront Functions): CloudFront Functions **不能** 访问 DynamoDB（无网络访问权限）。错。
    * **D:** DynamoDB Global Tables (多活) + CloudFront + **Lambda@Edge**。Lambda@Edge 可以选择最近的区域端点访问 DynamoDB，实现极低延迟。
* **锁定 D。**

**3. ✅ 正确选项解析 (选项 D)**
* **DynamoDB Global Tables:** 多区域多活写入。
* **Lambda@Edge:** 边缘计算，就近访问数据。

**5. 📚 核心考点:** 全球极低延迟应用架构 (Edge + Global Tables)。

---

#### 📝 [427/529] CloudFront 自定义头增强源站安全 (Secret Header)

**1. 🕵️‍♂️ 题眼与约束分析**
* **架构：** CloudFront -> ALB -> EC2。
* **漏洞：** ALB is Public (in public subnets).
* **需求：** **Enhance origin security** (防止绕过 CloudFront 直接访问 ALB)。

**2. ⚡ 秒杀思路**
* **方法 1：IP 白名单 (B/D)。**
    * 使用 Managed Prefix List (安全组) 或 WAF IP Set。
    * 选项 B 说 "Create WAF rule with IP match... move ALB to private subnets"。
        * ALB 必须在 Public Subnet 才能被 CloudFront 访问（除非 CloudFront 用了私有连接功能，但这很新且有限制，通常 ALB 是 Public 的）。如果把 ALB 移到 Private Subnet，CloudFront 就连不上了。B 错在移动 ALB。
        * D 说 "Shield Advanced... SG Allow CloudFront IPs"。Shield Advanced 是防 DDoS 的，虽然送 WAF，但 SG 规则更直接。但 CloudFront IP 范围很大。
* **方法 2：自定义头 (Custom Header) (A/C)。**
    * CloudFront 添加一个 **Secret Header** (e.g. `X-Secret-Token: random-string`)。
    * ALB (通过 WAF 或 Listener Rule) 检查这个头。如果没有或不对，拒绝。
    * 这样即使攻击者知道 ALB IP，没有 Token 也进不去。
    * **Secrets Manager vs Parameter Store:**
        * **A:** Secrets Manager + Lambda Rotation + WAF Rule (String Match)。
            * WAF 支持字符串匹配。Secrets Manager 支持自动轮换。CloudFront 支持添加头。
            * 这是一个非常安全的闭环：定期轮换密钥，WAF 自动更新规则（通过 Lambda），CloudFront 自动更新配置（通过 Lambda）。
        * **C:** Parameter Store + ALB Listener Rule? ALB Listener Rule 支持 header 检查。但 C 选项说 "Check value of custom header and block in ALB"。ALB Listener Rule 确实可以做。但 Parameter Store 的轮换机制不如 Secrets Manager 完善（需要自定义）。
    * **比较 A 和 C:** A 方案引入了 WAF，更加标准化和安全（WAF 是专门做请求过滤的）。而且 Secrets Manager 是凭证管理的最佳实践。C 在 ALB 上做 Block 也可以，但 A 的 "String Match Rule" 在 WAF 层面拦截更高效。
    * 更关键的是，**A 选项** 描述了一个完整的自动化轮换流程。
    * **另外：** 选项 B 的 IP 限制也是一种方法，但它错误地把 ALB 移到了私有子网。
    * **所以 A 是最佳的“增强安全性”方案（Shared Secret）。**
    * **锁定 A。**

**3. ✅ 正确选项解析 (选项 A)**
* **Custom Header (Shared Secret):** 确保请求来自 CloudFront。
* **Secrets Manager + WAF:** 自动化密钥轮换与验证。

**5. 📚 核心考点:** CloudFront Origin 安全加固 (Shared Secret pattern)。

---

#### 📝 [428/529] 全球多区域私有数据访问 (DX Gateway + Direct Connect)

**1. 🕵️‍♂️ 题眼与约束分析**
* **总部：** US。
* **数据：** Multiple AWS Regions (US + others)。
* **需求：**
    1.  Access data from Global WAN (总部访问全球 AWS 数据)。
    2.  **Traffic NOT over public internet** (不走公网)。
    3.  **Cost-effective**。
    4.  High Available.

**2. ⚡ 秒杀思路**
* **Direct Connect Gateway (DXGW):**
    * 这是一个允许**一个** Direct Connect 连接访问**所有** AWS 区域（除了中国区）VPC 的功能。
    * 你不需要为每个区域拉一条 DX 线 (A 错)。
    * 也不需要复杂的 VPC Peering 或 Transit VPC (B/C 错，虽然可行但成本高、跳数多)。
* **架构：**
    * 总部建立 2 条 DX 连接到美国区域（为了 HA）。
    * 创建 **Direct Connect Gateway**。
    * 将 DXGW 关联到所有区域的 VPC (Virtual Private Gateways)。
    * 这样，总部可以通过这两条 DX 线，经由 AWS 骨干网（DXGW），直接访问全球任何区域的 VPC。
    * $\rightarrow$ **选中 D**。
* **锁定 D。**

**3. ✅ 正确选项解析 (选项 D)**
* **Direct Connect Gateway:** 一点接入，全球通达（私有网络）。

**5. 📚 核心考点:** Direct Connect Gateway 的跨区域连接能力。

---

#### 📝 [429/529] 本地 VMware 灾备到 AWS (Elastic Disaster Recovery)

**1. 🕵️‍♂️ 题眼与约束分析**
* **源：** VMware vSphere VMs (Windows)。
* **数据：** Proprietary format (专有格式，必须通过 App 读取 -> 块级复制)。
* **需求：**
    1.  DR to AWS (Failover)。
    2.  **Failback** to on-prem (回切)。
    3.  **RPO 5 mins**。
    4.  **Minimal operational overhead**。

**2. ⚡ 秒杀思路**
* **工具选型：**
    * **AWS Elastic Disaster Recovery (DRS) (B):**
        * 专为 DR 设计。
        * 支持块级复制，RPO 秒级（满足 5 分钟）。
        * 支持 **Failback** 到本地（vCenter）。
        * 自动化编排恢复实例。
    * DataSync (A): 文件级，不适合“专有格式数据+OS状态”的整机 DR。
    * Storage Gateway (C): 只是数据备份，恢复需要手动重建 EC2，RTO 慢，Failback 复杂。
    * FSx (D): 文件系统，不解决应用服务器本身的 DR。
* **锁定 B。**

**3. ✅ 正确选项解析 (选项 B)**
* **AWS DRS:** 能够实现低 RPO/RTO 的整机灾备与回切。

**5. 📚 核心考点:** VMware 环境的 AWS 灾备标准方案 (DRS)。

---

#### 📝 [430/529] S3 跨区域上传延迟优化 (S3 Transfer Acceleration)

**1. 🕵️‍♂️ 题眼与约束分析**
* **中心桶：** eu-north-1 (Stockholm)。
* **新区域：** sa-east-1 (Sao Paulo), ap-northeast-1 (Tokyo)。
* **问题：** **Significant latency** uploading data to S3 in eu-north-1。
* **需求：** Improve latency。

**2. ⚡ 秒杀思路**
* **跨洲传输加速：**
    * 从南美/亚洲上传到欧洲，物理距离远，公网路由不稳定。
    * **S3 Transfer Acceleration (S3TA) (B):** 利用 CloudFront 边缘节点，走 AWS 优化骨干网上传。这是解决长距离上传延迟的标准方案。
* **其他选项：**
    * A (VPC Endpoint): 只是走内网，不改变物理距离和路由路径（如果是跨区域 VPC Endpoint 也是走骨干网，但 S3TA 更针对上传优化）。
    * C (CRR): 在每个区域建桶再复制？虽然上传快（本地上传），但数据到达中心桶还是有复制延迟（CRR 也是异步的）。而且这增加了架构复杂度（多桶管理）。题目问 "Improve latency of data arriving at S3 bucket" (指中心桶)。S3TA 直接加速到达中心桶的过程。如果是 C，数据到达中心桶的时间 = 本地上传时间 + CRR 时间。通常 S3TA 会比这快（或相当，但架构更简单）。
    * D (Multipart Upload): 主要是针对大文件吞吐，对延迟（Latency）本身改善有限（除非并发度极高掩盖了延迟）。但 Transfer Acceleration 是物理层面的优化。
    * **决胜：** 题目强调 "Global reach... significant latency"。**Transfer Acceleration** 是最对口的特性。
    * **锁定 B。**

**3. ✅ 正确选项解析 (选项 B)**
* **S3 Transfer Acceleration:** 利用全球边缘网络加速跨区域上传。

**5. 📚 核心考点:** S3 跨区域上传性能优化。

---
**小结：**
这组题目的 **Global Accelerator**、**S3 Transfer Acceleration**、**DRS Failback** 都是必考题。

**恭喜你，430 题！加油！**